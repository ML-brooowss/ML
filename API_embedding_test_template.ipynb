{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE86+vDF6Ut6shv6gkLejT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linneverh/ML/blob/main/API_embedding_test_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prep datasets"
      ],
      "metadata": {
        "id": "hjaDrZtL4T2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35hskhIG4Naj"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "interactions = pd.read_csv('https://raw.githubusercontent.com/linneverh/MachineLearning/main/interactions_train.csv')\n",
        "items1 = pd.read_csv(\"\")\n",
        "items2 = pd.read_csv(\"\")\n",
        "items = pd.concat([items1, items2])\n",
        "\n",
        "#rename columns\n",
        "interactions = interactions.rename(columns={'u': 'user_id', 'i': 'book_id', 't': 'timestamp'})\n",
        "items=items.rename(columns={'i':'book_id'})\n",
        "# Display the first rows of the updated interactions DataFrame\n",
        "display(interactions.head())\n",
        "display(items.head())\n",
        "\n",
        "# Display the first rows of each dataset\n",
        "display(interactions.head())\n",
        "display(items.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BASIC DEFINITIONS NEEDED"
      ],
      "metadata": {
        "id": "IYgdqafL7G-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to predict interactions based on item similarity\n",
        "def item_based_predict(interactions, similarity, epsilon=1e-9):\n",
        "    \"\"\"\n",
        "    Predicts user-item interactions based on item-item similarity.\n",
        "    Parameters:\n",
        "        interactions (numpy array): The user-item interaction matrix.\n",
        "        similarity (numpy array): The item-item similarity matrix.\n",
        "        epsilon (float): Small constant added to the denominator to avoid division by zero.\n",
        "    Returns:\n",
        "        numpy array: The predicted interaction scores for each user-item pair.\n",
        "    \"\"\"\n",
        "    # np.dot does the matrix multiplication. Here we are calculating the\n",
        "    # weighted sum of interactions based on item similarity\n",
        "    pred = similarity.dot(interactions.T) / (similarity.sum(axis=1)[:, np.newaxis] + epsilon)\n",
        "    return pred.T  # Transpose to get users as rows and items as columns"
      ],
      "metadata": {
        "id": "p81OvZsq7GTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendation frame generation\n",
        "def create_recommendation_table(user_predictions, top_n=10, separator=\" \"):\n",
        "    \"\"\"\n",
        "    Creates a table of top-N recommendations for each user.\n",
        "\n",
        "    Args:\n",
        "        user_predictions (numpy.ndarray): Rows = users, columns = items. Predicted scores.\n",
        "        top_n (int): Number of top recommendations per user.\n",
        "        separator (str): Delimiter to join recommended book IDs.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Columns = ['user_id', 'recommendation'].\n",
        "    \"\"\"\n",
        "    recommendations = []\n",
        "    num_users = user_predictions.shape[0]\n",
        "\n",
        "    for user_id in range(num_users):\n",
        "        top_items = np.argsort(user_predictions[user_id, :])[-top_n:][::-1]\n",
        "        recommendations.append({\n",
        "            'user_id': user_id,\n",
        "            'recommendation': separator.join(map(str, top_items))\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(recommendations)"
      ],
      "metadata": {
        "id": "jCteJpkE7KY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the precision_recall_at_k function\n",
        "def precision_recall_at_k(prediction, ground_truth, k=10):\n",
        "    \"\"\"\n",
        "    Calculates Precision@K and Recall@K for top-K recommendations.\n",
        "    Parameters:\n",
        "        prediction (numpy array): The predicted interaction matrix with scores.\n",
        "        ground_truth (numpy array): The ground truth interaction matrix (binary).\n",
        "        k (int): Number of top recommendations to consider.\n",
        "    Returns:\n",
        "        precision_at_k (float): The average precision@K over all users.\n",
        "        recall_at_k (float): The average recall@K over all users.\n",
        "    \"\"\"\n",
        "    num_users = prediction.shape[0]\n",
        "    precision_at_k, recall_at_k = 0, 0\n",
        "\n",
        "    for user in range(num_users):\n",
        "        # TODO: Get the indices of the top-K items for the user based on predicted scores\n",
        "        top_k_items = np.argsort(prediction[user, :])[-k:]\n",
        "\n",
        "        # TODO: Calculate the number of relevant items in the top-K items for the user\n",
        "        relevant_items_in_top_k = np.isin(top_k_items, np.where(ground_truth[user, :] == 1)[0]).sum()\n",
        "\n",
        "        # TODO: Calculate the total number of relevant items for the user\n",
        "        total_relevant_items = ground_truth[user, :].sum()\n",
        "\n",
        "        # Precision@K and Recall@K for this user\n",
        "        precision_at_k += relevant_items_in_top_k / k\n",
        "        recall_at_k += relevant_items_in_top_k / total_relevant_items if total_relevant_items > 0 else 0\n",
        "\n",
        "    # Average Precision@K and Recall@K over all users\n",
        "    precision_at_k /= num_users\n",
        "    recall_at_k /= num_users\n",
        "\n",
        "    return precision_at_k, recall_at_k"
      ],
      "metadata": {
        "id": "oTQBqsYZ7T4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### API Embeddings tests\n",
        "Try the different embeddings - what gives best result?"
      ],
      "metadata": {
        "id": "jtQEJFnf5suc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the single embedding column that already holds the full vector\n",
        "embedding_col = 'combined_embedding'  # replace with actual column name if different\n",
        "\n",
        "# Parse the embedding strings into numpy arrays\n",
        "items[embedding_col] = items[embedding_col].apply(lambda x: np.fromstring(x.strip('[]'), sep=','))\n",
        "\n",
        "# Drop rows with missing or malformed embeddings\n",
        "valid_items = items[items[embedding_col].notna()].reset_index(drop=True)\n",
        "\n",
        "# Stack all embeddings into a matrix\n",
        "embedding_matrix = np.vstack(valid_items[embedding_col].values)\n",
        "\n",
        "# Compute item-item cosine similarity\n",
        "embedding_sim = cosine_similarity(embedding_matrix)\n",
        "\n",
        "# Optional: normalize the similarity matrix row-wise\n",
        "embedding_sim = normalize(embedding_sim)"
      ],
      "metadata": {
        "id": "kGSYDLcu5rkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the item-based predictions for positive interactions\n",
        "item_EMBED_prediction = item_based_predict(train_data_matrix, embedding_sim)\n",
        "print(\"Predicted Interaction Matrix:\")\n",
        "print(item_EMBED_prediction)\n",
        "print(item_EMBED_prediction.shape)"
      ],
      "metadata": {
        "id": "d8Z809ch7Chm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK PRECISION & RECALL NOT YET WITH CROSS-VALIDATION [OVERFITTING PROBLEM THOUGH]\n",
        "precision_item_k, recall_item_k = precision_recall_at_k(item_prediction, test_data_matrix, k=10)\n",
        "\n",
        "#print('User-based CF Precision@K:', precision_user_k)\n",
        "#print('User-based CF Recall@K:', recall_user_k)\n",
        "print('Item-based CF Precision@K:', precision_item_k)\n",
        "print('Item-based CF Recall@K:', recall_item_k)"
      ],
      "metadata": {
        "id": "NXhm78Oj7gIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09X0XkGh8CNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}